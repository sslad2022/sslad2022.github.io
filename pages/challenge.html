<!DOCTYPE html>
<html lang="en">

<head>
  <title>Challenge | The 2nd Workshop on Self-supervised Learning for Next-Generation Industry-level Autonomous Driving </title>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
  <link rel="stylesheet" href="../theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="../theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="../theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="../theme/css/style.css">
  <link rel="stylesheet" href="../theme/css/custom.css">
  <link href="../theme/fonts/icomoon/style.css" rel="stylesheet">
  <link href="../theme/css2/style.css" rel="stylesheet">

  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-88572407-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body>
  <header class="header">
    <div class="container">  
      <div class="nav-bar">
        <div class="nav-item"><a href="../index.html">Home</a></div>
        <div class="nav-item"><a href="../pages/call-for-participation.html">Call for Submissions</a></div>
        <div class="nav-item"><a href="../pages/schedule.html">Schedule</a></div>
        <div class="nav-item"><a href="../pages/speakers.html">Speakers</a></div>
        <div class="nav-item"><a href="../pages/organizers.html">Organizers</a></div>
        <div class="nav-item"><a href="../pages/Program Committee.html">Program Committee</a></div>
        <div class="nav-item active"><a href="../pages/challenge.html">Challenge</a></div>
        <div class="nav-item"><a href="../pages/Accepted Paper.html">Accepted Papers</a></div>
        <div class="nav-item"><a href="https://sslad2021.github.io/index.html">2021</a></div>
      </div>
      <img src="../images/title.png">
    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1 class="title">Challenges</h1>
      <div class="row middle">
        <div class="col-lg-12" id="challenge1">
          <div>
            <h2 class="title">
              <div>Track1</div>
              <div class="sub-title">2D object detection</div>
            </h2>
          </div>
          <div class="trend-entry d-flex">
            <div class="trend-contents">
              <p>
                For 2D object detection, we provide a real-word training dataset with 10 million images of which 5K are labeled and with 5K/10K validation/testing labeled images for evaluation. This dataset has been collected throughout diverse scenarios in cities in China and contains scenes in a wide variety of places, objects and weather conditions such as highways, city streets, country roads, rainy weather, also different cameras / camera setups.
              </p>

              <ul>
                <li><strong>Evaluation:</strong> Leaderboard ranking for this track is by Mean Average Precision(mAP) among all categories, that is, the mean over the APs of pedestrian, cyclist, car, truck, tram and tricycle. The IoU overlap threshold for pedestrian, cyclist, tricycle is set to 0.5, and for car, truck, tram is set to 0.7. Only camera images of SODA10M are allowed to be used.</li>
                <li><strong>Dataset: </strong> Please refer to <a href="https://soda-2d.github.io/"> SODA-2d </a> for detailed dataset introduction and dataset downloads. <br /></li>
                <li><strong>Submission: The challenge is now available on <a href="https://codalab.lisn.upsaclay.fr/competitions/6163">codalab</a>.</strong></li>
                </li>
              </ul>
              <div><h5>Technique report:</h5></div>
              <ul>
                <li>Technique report for Frist Prize  <a href="../files/track%201%202D%20object%20detection/1st%20Place%20Solution_for_SSLAD_Challenge_2022__2D_Object_Detection.pdf">Download</a></li>
                <li>Technique report for Second Prize  <a href="../files/track%201%202D%20object%20detection/2nd%20Place%20Solution%20Two_stage_Detector_Provides_Better_Knowledge_for_YOLO_in_Semi_supervised_Object_Detection.pdf">Download</a></li>
                <li>Technique report for Third Prize  <a href="../files/track%201%202D%20object%20detection/3rd%20Palce%20Solution%20for%20ECCV%202022%20Workshop%20SSLAD%20Track%201.pdf">Download</a></li>
              </ul>
            </div>
          </div>
        </div>

        <div class="col-lg-12" id="challenge2">
          <div>
            <h2 class="title">
              <div>Track2</div>
              <div class="sub-title">3D object detection</div>
            </h2>
          </div>
          <div class="trend-entry d-flex">
            <div class="trend-contents">
              <p>
                For 3D object detection, we provide a large-scale dataset with 1 million point clouds and 7 million images. We annotated 5K, 3K and 8K scenes for training, validation and testing set respectively and leave the other scenes unlabeled. We provide 3D bounding boxes for car, cyclist, pedestrian, truck and bus.
              </p>

              <ul>
                <li><strong>Evaluation:</strong> Leaderboard ranking for this track is by Mean Average Precision with Heading (mAPH) / L2 among "ALL_NS" (all Object Types except signs), that is, the mean over the APHs of car, cyclist, pedestrian, truck and bus. All sensors are allowed to be used. </li>
                <li><strong>Dataset: </strong> Please refer to <a href="https://once-for-auto-driving.github.io/"> ONCE </a> for detailed dataset introduction and dataset downloads. </li>
                <li><strong>Submission: The challenge is now available on <a href="https://codalab.lisn.upsaclay.fr/competitions/6673">codalab</a>.</strong></li>
              </ul>
              <div><h5>Technique report:</h5></div>
              <ul>
                <li>Technique report for Frist Prize  <a href="../files/track%202%203D%20object%20detection/1st%20Place%20Solution%20to%20the%203D%20Object%20Detection.pdf">Download</a></li>
                <li>Technique report for Second Prize  <a href="../files/track%202%203D%20object%20detection/2nd%20Place%20Solution%20SHULab_2nd_SSLAD_ECCV_Workshop_Tech_Report.pdf">Download</a></li>
                <li>Technique report for Third Prize  <a href="../files/track%202%203D%20object%20detection/3rd%20Place%20Solution%20to%20SSLAD%20Challenge%20-%203D.pdf">Download</a></li>
              </ul>
            </div>
          </div>
        </div>

        <div class="col-lg-12" id="challenge3">
          <div>
            <h2 class="title">
              <div>Track3</div>
              <div class="sub-title">Corner Case Detection</div>
            </h2>
          </div>
          <div class="trend-entry d-flex">
            <div class="trend-contents">
              <p>
                Deep learning has achieved prominent success in detecting common traffic participants (e.g., cars, pedestrians, and cyclists). Such detectors, however, are generally incapable of detecting novel objects that are not seen or rarely seen in the training process. These objects are called (object-level) corner cases, which consist of two categories, namely 1) instance of novel class (e.g., a runaway tire) and 2) novel instance of common class (e.g., an overturned truck). Properly dealing with corner cases has become one of the essential keys to reliable autonomous-driving perception systems. The aim of this challenge is to discover novel methods for detecting corner cases among common traffic participants in the real world.
              </p>
              <p>
                For this challenge, we allow [SODA10M](https://soda-2d.github.io/), [ONCE](https://once-for-auto-driving.github.io/), and ImageNet-1k for training/pretraining. The evaluation will be conducted on the corner case dataset, CODA2022, which contains 9768 camera images with 80180 annotated objects spanning 43 object categories.
              </p>
              <p>
                The first 7 categories (pedestrian, cyclist, car, truck, tram, tricycle, bus) are common categories, while the rest are novel categories.
              </p>

              <ul>
                <li><strong>Evaluation:</strong> For this task, we define a custom metric which is the sum of the four metrics as follows: 
                  <ul>
                    <li>AP-common: mAP over objects of common categories;</li>
                    <li>AP-agnostic: mAP over objects of all categories in a class-agnostic manner;</li>
                    <li>AR-agnostic: mAR over objects of all categories in a class-agnostic manner;</li>
                    <li>AR-agnostic-corner: mAR over corner-case objects of all categories in a class-agnostic manner;</li>
                  </ul>
                  where mAP and mAR stand for mean Average Precison and mean Average Recall in COCO API.
                </li>
                <li><strong>Dataset: </strong> Please refer to <a href="https://soda-2d.github.io/">SODA10M</a> and <a href="https://once-for-auto-driving.github.io/">ONCE</a> for detailed dataset introduction and dataset downloads. CODA2022 can be downloaded on the submission page once the challenge begins.</li>
                <li><strong>Submission: Please follow the instructions of the challenge published at <a href="https://codalab.lisn.upsaclay.fr/competitions/6639"> CodaLab Competitions</a>.</strong></li>
              </ul>
              <div><h5>Technique report:</h5></div>
              <ul>
                <li>Technique report for Frist Prize  <a href="../files/track%203%20corner%20case/1st_for_SSLAD2022_Corner_Case_Detection.pdf">Download</a></li>
                <li>Technique report for Second Prize  <a href="../files/track%203%20corner%20case/2nd_Learn_to_Detect_Corner_Case_with_Semi_supervised_Learning.pdf">Download</a></li>
                <li>Technique report for Third Prize  <a href="../files/track%203%20corner%20case/3rd_place_technical_report.pdf">Download</a></li>
              </ul>
            </div>
          </div>
      </div>

        <div class="col-lg-12" id="challenge4">
          <div>
            <h2 class="title">
              <div>Track4</div>
              <div class="sub-title">Multiple object tracking and segmentation</div>
            </h2>
          </div>
          <div class="trend-entry d-flex">
            <div class="trend-contents">
              <p>
                This is a large-scale tracking challenge under the most diverse driving conditions. Understanding the temporal association and shape of objects within videos is one of the fundamental yet challenging tasks for autonomous driving. The BDD100K MOT and MOTS datasets provides diverse driving scenarios with high quality instance segmentation masks under complicated occlusions and reappearing patterns, which serves as a great testbed for the reliability of the developed tracking and segmentation algorithms in real scenes. The BDD100K dataset also include 100K raw video sequences, which can be readily used for self-supervised learning. We hope the utilization of large-scale unlabeled video data in self-driving could further boost the performance of MOT & MOTS. In this challenge, we provide two tracks: (1) Main track - standard MOT and MOTS, and (2) Teaser track - self-supervised MOT and MOTS.
              </p>

              <ul>
                <li><strong>Evaluation:</strong> We use standard MOT and MOTS evaluation metrics. </li>
                <li><strong>Dataset: </strong> Please refer to our <a href='https://www.bdd100k.com/challenges/eccv2022/'>BDD100K Challenge Page</a> for details regarding the dataset..
                </li>
                <li><strong>Submission: <a href="https://eval.ai/web/challenges/challenge-page/1836">MOT evaluation server</a>/<a href="https://eval.ai/web/challenges/challenge-page/1834">MOTS evaluation server</a> </strong></li>
              </ul>
              <div><h5>Technique report:</h5></div>
              <ul>
                <li>Technique report for Frist Prize in track MOT and SSMOT  <a href="../files/track%204%20tech-reports-bdd100k-eccv22/1st-MOT-MOTS-SSMOT-SSMOTS.pdf">Download</a></li>
                <li>Technique report for Second Prize for in track MOT  <a href="../files/track%204%20tech-reports-bdd100k-eccv22/2nd-MOT.pdf">Download</a></li>
                <li>Technique report for Third Prize for in track MOT  <a href="../files/track%204%20tech-reports-bdd100k-eccv22/3rd-MOT-2nd-MOTS.pdf">Download</a></li>
                <li>Technique report for Frist Prize in track MOTS and SSMOTS  <a href="../files/track%204%20tech-reports-bdd100k-eccv22/1st-MOT-MOTS-SSMOT-SSMOTS.pdf">Download</a></li>
                <li>Technique report for Second Prize for in track MOTS  <a href="../files/track%204%20tech-reports-bdd100k-eccv22/3rd-MOT-2nd-MOTS.pdf">Download</a></li>
                <li>Technique report for Third Prize for in track MOTS  <a href="../files/track%204%20tech-reports-bdd100k-eccv22/3rd-MOTS.pdf">Download</a></li>
              </ul>
            </div>
          </div>
        </div>

        <div class="col-lg-12" id="challenge5">
          <div>
            <h2 class="title">
              <div>Track5</div>
              <div class="sub-title">Unified model for multi-task learning</div>
            </h2>
          </div>
          <div class="trend-entry d-flex">
            <div class="trend-contents">
              <p>
                Conception system for autonomous driving is responsible for providing various information based on sensor collect data, like the position of other traffic participants, signals of traffic lights and signs, and etc.. Currently, these information is provided by independent models, which requires great computation resources and neglects the latent connections between these tasks. Therefore it is intuitive to combine these tasks together during training process, i.e., Multi-Task Learning.  
              </p>
              <p>We provide mutli-task learning track as part of this year's challenge. In this track, we provide a real world collected dataset with 3000+ frame data. Each frame contains 1 point cloud and 7 images along with annotation of 3D object detection, lane detection and road segmentation.</p>
              <ul>
                <li><strong>Evaluation:</strong> We use common evaluation metrics of each tasks, i.e., mAP for object detectiom, mIoU for road segmentation.</li>
                <li><strong>Dataset: </strong> A new multi-task learning dataset called <a href="https://drive.google.com/file/d/1LntvctUbdipIZqp64oovuGQ9XH5v9PqJ/view?usp=sharing">AutoScenes</a> / <a href="https://drive.google.com/file/d/1qkeks40H-U7iikNbDie0RoHOFHJAIr4b/view?usp=sharing">AutoScenes annotation</a> is released. </li>
                <li><strong>Submission: The challenge is now available on <a href="https://codalab.lisn.upsaclay.fr/competitions/6674">codalab</a>. </strong></li>
              </ul>
              <div><h5>Technique report:</h5></div>
              <ul>
                <li>Technique report for Frist Prize  <a href="../files/track%205%20MULTI-TASK%20LEARNING/1st%20Place%20Solution%20Tech_Report_SSLAD2022_Track5_ECCV2022.pdf">Download</a></li>
              </ul>
            </div>
          </div>
        </div>

        <div class="col-lg-12" id="awards">
          <br />
          <div>
            <h3 class="title">CHALLENGE PRIZES: (TOTAL 50,000 USD)</h3>
          </div>
          <div class="trend-entry d-flex">
            <div class="trend-contents">
              <p>
                Challenge participants with the most successful and innovative entries will be invited to present at this workshop and will receive awards. There are 10,000 USD cash prize for each track. A 5,000 USD cash prize will be awarded to the top performers in each task and 2nd will be awarded with 3,000 USD and 3rd will be awarded with 2,000 USD.
              </p>
            </div>
          </div>
        </div>

        <!-- References -->
        <div class="col-lg-12" id="reference">
        <br />
          <div>
            <h3 class="title">References</h3>
          </div>
          <div class="trend-entry d-flex">
            <div class="trend-contents">
              <p>[1] Han J, Liang X, Xu H, et al. SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving[J]. arXiv preprint arXiv:2106.11118, 2021.</p>
              <p>[2] Mao J, Niu M, Jiang C, et al. One Million Scenes for Autonomous Driving: ONCE Dataset[J]. arXiv preprint arXiv:2106.11037, 
            </div>
          </div>
        </div>

        <!-- Spanner Area Start-->
        <div class="col-lg-12 with-img" id="spanner">
          <br />
          <div>
            <h3 class="title">Sponsor</h3>
          </div>
          <div class="trend-entry display-flex display-flex-center">
            <!-- pic No.2 -->
            <div class="col-xs-6 col-md-4 spanner-pic-box">
              <div class="thumbnail">
                <a href="https://www.huawei.com/cn/">
                  <img src="../images/sponsor/huawei.png" alt="Huawei"
                    class="spanner-img" align="center">
                  <div>
                    <h5>Huawei</h5>
                  </div>
                </a>
              </div>
            </div>

            <div class="col-xs-6 col-md-4 spanner-pic-box">
              <div class="thumbnail">
                <a href="http://www.sysu.edu.cn/cn/index.htm">
                  <img src="../images/sponsor/sysu.jpeg" alt="Sun Yat-sen University" style="height: 100px", align="center">
                  <div>
                    <h5>Sun Yat-sen University</h5>
                  </div>
                </a>
              </div>
            </div>
            
            <div class="col-xs-6 col-md-4 spanner-pic-box">
              <div class="thumbnail" style="
    height: 100px;
    vertical-align: middle;
    margin-bottom: 0px;
    padding-top: 30px;
">
                <a href="https://ethz.ch/en.html" style="
    /* height: 100px; */
">
                  <img src="../images/sponsor/ETH.png" alt="ETH Zürich" style="/* height: 100px; */width: 200px;" ,="" align="center">
                  
                </a>
              </div>
              <div>
                    <h5>ETH Zürich</h5>
                  </div>
            </div>

            <div class="col-xs-6 col-md-4 spanner-pic-box">
              <div class="thumbnail">
                <a href="https://www.noahlab.com.hk/#/home">
                  <img src="../images/sponsor/noah.png" alt="Noah's Ark LAB" style="height: 100px", align="center">
                  <div>
                    <h5>NOAH'S ARK LAB</h5>
                  </div>
                </a>
              </div>
            </div>

            <div class="col-xs-6 col-md-4 spanner-pic-box">
              <div class="thumbnail">
                <a href="">
                  <img src="../images/sponsor/ads.png" alt="Autonomous Driving Solution" style="height: 100px", align="center">
                  <div>
                    <h5>Autonomous Driving Solution</h5>
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <br /><br />

  </div>
  <!-- .site-wrap -->
  <!-- loader -->
  <div class="show fullscreen" id="loader">
    <svg class="circular" height="48px" width="48px">
      <circle class="path-bg" cx="24" cy="24" fill="none" r="22" stroke="#eeeeee" stroke-width="4" />
      <circle class="path" cx="24" cy="24" fill="none" r="22" stroke="#ff5e15" stroke-miterlimit="10"
        stroke-width="4" />
    </svg>
  </div>

    <div class="footer">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <div class="copyright">
              <p>
                <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                Copyright &copy;
                <script>document.write(new Date().getFullYear());</script>
                All rights reserved | This template is made with <i aria-hidden="true"
                  class="icon-heart text-danger"></i> by
                <a href="https://colorlib.com" target="_blank" style="text-decoration: none;">Colorlib</a>
                <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>


  <script src="../theme/js/jquery-3.3.1.min.js"></script>
  <script src="../theme/js/jquery-migrate-3.0.1.min.js"></script>
  <script src="../theme/js/jquery-ui.js"></script>
  <script src="../theme/js/popper.min.js"></script>
  <script src="../theme/js/bootstrap.min.js"></script>
  <script src="../theme/js/owl.carousel.min.js"></script>
  <script src="../theme/js/jquery.stellar.min.js"></script>
  <script src="../theme/js/jquery.countdown.min.js"></script>
  <script src="../theme/js/bootstrap-datepicker.min.js"></script>
  <script src="../theme/js/jquery.easing.1.3.js"></script>
  <script src="../theme/js/aos.js"></script>
  <script src="../theme/js/jquery.fancybox.min.js"></script>
  <script src="../theme/js/jquery.sticky.js"></script>
  <script src="../theme/js/jquery.mb.YTPlayer.min.js"></script>


  <script src="../theme/js/main.js"></script>

</body>

</html>
